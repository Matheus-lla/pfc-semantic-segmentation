{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting and Scan Line Run for 3D LiDAR\n",
    "\n",
    "Ground Plane Fitting (GPF) and Naive Baseline for 3D LiDAR Segmentation\n",
    "\n",
    "This notebook implements ground segmentation using the Ground Plane Fitting (GPF) algorithm \n",
    "proposed in:\n",
    "\n",
    "\"Fast Segmentation of 3D Point Clouds: A Paradigm on LiDAR Data for Autonomous Vehicle Applications\"\n",
    "by D. Zermas, I. Izzat, and N. Papanikolopoulos, 2017.\n",
    "\n",
    "The implementation also includes a naive baseline method for comparison, as well as \n",
    "basic clustering and visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp gpf_slr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting (GPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_initial_seed_indices(\n",
    "    point_cloud: np.ndarray, num_points: int = 1000, height_threshold: float = 0.4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract initial seed points for ground plane estimation (GPF).\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 array of points (x, y, z).\n",
    "        num_points (int): number of lowest Z points to average as LPR.\n",
    "        height_threshold (float): threshold to select seeds close to LPR height.\n",
    "\n",
    "    Returns:\n",
    "        seeds_ids (np.ndarray): indices of points selected as initial seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the point cloud by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])  # Get indices sorted by height\n",
    "    sorted_points = point_cloud[sorted_indices]  # Apply sorting\n",
    "\n",
    "    # Step 2: Compute LPR (Lowest Point Representative)\n",
    "    lpr_height = np.mean(sorted_points[:num_points, 2])\n",
    "\n",
    "    # Step 3: Select point ids that are within threshold distance from LPR\n",
    "    mask = sorted_points[:, 2] < (lpr_height + height_threshold)\n",
    "    return sorted_indices[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def estimate_ground_plane(points: np.ndarray) -> \"tuple[np.ndarray, float]\":\n",
    "    \"\"\"\n",
    "    Estimate the ground plane parameters using Singular Value Decomposition (SVD).\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): N x 3 array (x, y, z) of seed points assumed to be on or near the ground.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane.\n",
    "            - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute centroid of the seed points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    centered_points = points - centroid\n",
    "\n",
    "    # Step 2: Compute the covariance matrix of centered points\n",
    "    covariance_matrix = np.cov(centered_points.T)\n",
    "\n",
    "    # Step 3: Perform SVD on the covariance matrix to extract principal directions\n",
    "    _, _, vh = np.linalg.svd(covariance_matrix)\n",
    "\n",
    "    # Step 4: Normal vector is the direction with smallest variance (last column of V^T)\n",
    "    normal = vh[-1]\n",
    "\n",
    "    # Step 5: Compute plane bias using point-normal form: ax + by + cz + d = 0\n",
    "    d = -np.dot(normal, centroid)\n",
    "\n",
    "    return (normal, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def refine_ground_plane(\n",
    "    point_cloud: np.ndarray,\n",
    "    num_points: int = 1000,\n",
    "    height_threshold: float = 0.4,\n",
    "    distance_threshold: float = 0.2,\n",
    "    num_iterations: int = 5,\n",
    ") -> \"tuple[np.ndarray, tuple[np.ndarray, float]]\":\n",
    "    \"\"\"\n",
    "    Iteratively refine the ground plane estimation using seed points and distance threshold.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "        num_points (int): Number of lowest Z points used to compute the initial ground seed height (LPR).\n",
    "        height_threshold (float): Vertical distance threshold from the LPR used to select initial seed points.\n",
    "        distance_threshold (float): Max allowed point-to-plane distance for a point to be considered ground.\n",
    "        num_iterations (int): Number of iterations to refine the plane and ground classification.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id], input array with ground points labeled.\n",
    "            - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane.\n",
    "            - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 0: Use only XYZ for plane estimation\n",
    "    xyz = point_cloud[:, :3]\n",
    "\n",
    "    # Step 1: Get initial seed points based on lowest Z values\n",
    "    seed_indices = extract_initial_seed_indices(xyz, num_points, height_threshold)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Step 2: Estimate ground plane using current seeds\n",
    "        normal, d = estimate_ground_plane(xyz[seed_indices])\n",
    "\n",
    "        # Step 3: Compute distances from all points to the estimated plane\n",
    "        distances = np.abs(np.dot(xyz, normal) + d) / np.linalg.norm(normal)\n",
    "\n",
    "        # Step 4: Classify as ground if within distance threshold\n",
    "        is_ground = distances < distance_threshold\n",
    "\n",
    "        # Step 5: Update seeds with newly classified ground points\n",
    "        seed_indices = np.where(is_ground)[0]\n",
    "\n",
    "    # Final ground classification using last iteration's result\n",
    "    point_cloud[seed_indices, 4] = 9  # Set label = 9 for ground\n",
    "\n",
    "    return (point_cloud, (normal, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Line Run (SLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def group_by_scanline(point_cloud: np.ndarray) -> \"list[np.ndarray]\":\n",
    "    \"\"\"\n",
    "    Group points by their scanline index in a vectorized way.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: List of arrays. Each array contains the points (N_i x 6)\n",
    "                          from one scanline, sorted by scanline_id.\n",
    "    \"\"\"\n",
    "    scan_ids = point_cloud[:, 5].astype(int)\n",
    "    unique_ids = np.unique(scan_ids)\n",
    "\n",
    "    return [point_cloud[scan_ids == s_id] for s_id in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def find_runs(\n",
    "    scanline_points: np.ndarray, distance_threshold: float = 0.5\n",
    ") -> \"list[np.ndarray]\":\n",
    "    \"\"\"\n",
    "    Identify runs within a single scanline based on distance between consecutive points.\n",
    "\n",
    "    Args:\n",
    "        scanline_points (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "        distance_threshold (float): Distance threshold to consider two points part of the same run.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: List of arrays where each array contains the points of a run.\n",
    "    \"\"\"\n",
    "    num_points = len(scanline_points)\n",
    "    runs = []\n",
    "    current_run_indices = [0]  # start with the index of the first point\n",
    "\n",
    "    for i in range(1, num_points):\n",
    "        dist = np.linalg.norm(scanline_points[i, :3] - scanline_points[i - 1, :3])\n",
    "        if dist < distance_threshold:\n",
    "            current_run_indices.append(i)\n",
    "        else:\n",
    "            runs.append(scanline_points[current_run_indices])\n",
    "            current_run_indices = [i]\n",
    "\n",
    "    # append the last run\n",
    "    runs.append(scanline_points[current_run_indices])\n",
    "\n",
    "    # Check if first and last points are close (circular case)\n",
    "    circular_dist = np.linalg.norm(scanline_points[0, :3] - scanline_points[-1, :3])\n",
    "    # Only merge runs if:\n",
    "    # - the scanline appears to be circular (first and last points are close), and\n",
    "    # - there is more than one run (otherwise merging doesn't make sense)\n",
    "    if circular_dist < distance_threshold and len(runs) > 1:\n",
    "        # Merge last run with the first\n",
    "        runs[0] = np.vstack((runs[-1], runs[0]))\n",
    "        runs.pop()\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_labels(\n",
    "    runs_current: \"list[np.ndarray]\",\n",
    "    runs_above: \"list[np.ndarray]\",\n",
    "    label_equivalences: dict,\n",
    "    merge_threshold: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Update labels of current scanline runs based on proximity to runs from previous scanline using KDTree.\n",
    "\n",
    "    Args:\n",
    "        runs_current (list[np.ndarray]): List of N x 6 arrays for current scanline runs.\n",
    "        runs_above (list[np.ndarray]): List of N x 6 arrays for previous scanline runs.\n",
    "        label_equivalences (dict): Dictionary of label equivalences.\n",
    "        merge_threshold (float): Maximum distance to consider connection between runs.\n",
    "    \"\"\"\n",
    "\n",
    "    def resolve_label(label: int) -> int:\n",
    "        \"\"\"Find the final label by following the equivalence chain.\"\"\"\n",
    "        while label != label_equivalences[label]:\n",
    "            label = label_equivalences[label]\n",
    "        return label\n",
    "\n",
    "    def assign_new_label(run, label_equivalences, global_label_counter):\n",
    "        while global_label_counter == 9 or global_label_counter in label_equivalences:\n",
    "            global_label_counter += 1\n",
    "        run[:, 4] = global_label_counter\n",
    "        label_equivalences[global_label_counter] = global_label_counter\n",
    "        return global_label_counter + 1\n",
    "\n",
    "    def inherit_and_unify_labels(run, neighbor_labels, label_equivalences):\n",
    "        min_label = min(neighbor_labels)\n",
    "        run[:, 4] = min_label\n",
    "        for lbl in neighbor_labels:\n",
    "            label_equivalences[lbl] = min_label\n",
    "\n",
    "    global_label_counter = max(label_equivalences.values()) + 1\n",
    "\n",
    "    points_above = np.vstack(runs_above)\n",
    "    tree_above = KDTree(points_above[:, :3])  # use only x, y, z\n",
    "\n",
    "    for run in runs_current:\n",
    "        neighbor_labels = set()\n",
    "        dists, indices = tree_above.query(run[:, :3], k=1)\n",
    "        close_mask = dists[:, 0] < merge_threshold\n",
    "        close_indices = indices[close_mask, 0]\n",
    "        if close_indices.size > 0:\n",
    "            for idx in close_indices:\n",
    "                neighbor_label = points_above[idx, 4]\n",
    "                resolved_label = resolve_label(neighbor_label)\n",
    "                neighbor_labels.add(resolved_label)\n",
    "        if not neighbor_labels:\n",
    "            global_label_counter = assign_new_label(\n",
    "                run, label_equivalences, global_label_counter\n",
    "            )\n",
    "        else:\n",
    "            inherit_and_unify_labels(run, neighbor_labels, label_equivalences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_clusters(\n",
    "    scanlines: \"list[np.ndarray]\", label_equivalences: dict\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply resolved labels to all points and return a unified point cloud.\n",
    "\n",
    "    Args:\n",
    "        scanlines (list[np.ndarray]): List of N x 6 arrays for each scanline.\n",
    "        label_equivalences (dict): Dictionary of final label equivalences.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: N x 6 array with updated labels in column 4.\n",
    "    \"\"\"\n",
    "    non_ground_points = np.vstack(scanlines)\n",
    "\n",
    "    for idx in range(0, len(non_ground_points)):\n",
    "        non_ground_points[idx][4] = label_equivalences[non_ground_points[idx][4]]\n",
    "\n",
    "    return non_ground_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def scan_line_run_clustering(\n",
    "    point_cloud: np.ndarray,\n",
    "    distance_threshold: float = 0.5,\n",
    "    merge_threshold: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform scan line run clustering on non-ground points (predicted_label == 0).\n",
    "\n",
    "    This function detects connected components (runs) within scanlines, propagates\n",
    "    and merges labels across scanlines, and assigns final labels to each point.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, predicted_label, scanline_index].\n",
    "        distance_threshold (float): Distance threshold to consider two points part of the same run.\n",
    "        merge_threshold (float): Maximum distance to consider connection between runs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Point cloud with updated predicted labels (column 4).\n",
    "    \"\"\"\n",
    "    label_counter = 0\n",
    "    label_equivalences = {}\n",
    "\n",
    "    # Filter non-ground points (predicted_label == 0)\n",
    "    non_ground_mask = point_cloud[:, 4] == 0\n",
    "    non_ground_indices = np.nonzero(non_ground_mask)[0]  # ← Adicionada\n",
    "    non_ground_points = point_cloud[non_ground_mask].copy()\n",
    "    ground_points = point_cloud[~non_ground_mask]\n",
    "\n",
    "    if non_ground_points.size == 0:\n",
    "        raise ValueError(\"Point cloud already clustered or no non-ground points found.\")\n",
    "    # Group points into scanlines\n",
    "    scanlines = group_by_scanline(non_ground_points)\n",
    "\n",
    "    # Initialize clustering with the first scanline\n",
    "    runs_above = find_runs(scanlines[0], distance_threshold)\n",
    "    for runs in runs_above:\n",
    "        label_counter += 1\n",
    "        if label_counter == 9:  # reserve label 9 for ground\n",
    "            label_counter += 1\n",
    "        runs[:, 4] = label_counter\n",
    "        label_equivalences[label_counter] = label_counter\n",
    "\n",
    "    scanlines[0] = np.vstack(runs_above)\n",
    "\n",
    "    # Propagate labels through remaining scanlines\n",
    "    for i in range(1, len(scanlines)):\n",
    "        runs_current = find_runs(scanlines[i], distance_threshold)\n",
    "        update_labels(runs_current, runs_above, label_equivalences, merge_threshold)\n",
    "\n",
    "        scanlines[i] = np.vstack(runs_current)\n",
    "        runs_above = runs_current\n",
    "\n",
    "    clustered_points = extract_clusters(scanlines, label_equivalences)\n",
    "    point_cloud[non_ground_indices, 4] = clustered_points[:, 4]\n",
    "    return point_cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
