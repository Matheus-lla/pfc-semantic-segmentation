# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_utils.ipynb.

# %% auto 0
__all__ = ['SemanticKittiDataset', 'pad_collate_fn', 'PointCloudVisualizer', 'run_viz', 'main_viz', 'save_clusters',
           'verificar_consistencia_labels', 'get_statistics', 'compute_cluster_features',
           'extract_features_from_clusters', 'plot_1', 'plot_2', 'plot_3', 'plot_4', 'plot_5']

# %% ../nbs/00_utils.ipynb 2
import json
import os
import shutil
import subprocess
import sys
import threading
from collections import Counter, defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
import torch
from matplotlib import cm
from sklearn.decomposition import PCA
from torch.utils.data import Dataset

plt.style.use("seaborn-v0_8-whitegrid")  # ou "ggplot", "seaborn-paper", etc.

plt.rcParams.update(
    {
        "font.size": 16,
        "axes.titlesize": 20,
        "axes.labelsize": 18,
        "legend.fontsize": 14,
        "xtick.labelsize": 14,
        "ytick.labelsize": 14,
        "font.family": "serif",  # ou "Times New Roman"
    }
)
import numpy as np
import open3d as o3d
import yaml
import random

# %% ../nbs/00_utils.ipynb 4
class SemanticKittiDataset(Dataset):
    def __init__(
        self,
        data_path: str,
        split: str = "train",
        load_cluster: bool = False,
        return_np_array: bool = True,
        sequence_list: list[int] | None = None,
    ) -> None:
        """
        Initialize dataset loader.

        Args:
            data_path (str or Path): Base path to the SemanticKITTI dataset.
            split (str): Dataset split to use ('train', 'valid', or 'test').
        """
        self.data_path: Path = Path(data_path)
        self.split: str = split
        self.is_test: bool = split == "test"
        self.load_cluster: bool = load_cluster
        self.return_np_array = return_np_array

        # Paths to YAML config and data folders
        self.yaml_path: Path = Path("../experiments/semantic-kitti.yaml")
        # self.yaml_path: Path = self.data_path / 'semantic-kitti.yaml'
        self.velodynes_path: Path = (
            self.data_path / "data_odometry_velodyne/dataset/sequences"
        )
        self.labels_path: Path = (
            self.data_path / "data_odometry_labels/dataset/sequences"
        )
        self.clusters_path: Path = (
            self.data_path / "data_odometry_clusters/dataset/sequences"
        )

        # Load dataset metadata and label mappings
        with open(self.yaml_path, "r") as file:
            metadata: dict = yaml.safe_load(file)

        self.sequences: list[int] = sequence_list if sequence_list is not None else metadata["split"][split]
        self.learning_map: dict[int, int] = metadata["learning_map"]

        # Convert label map to numpy for fast lookup
        max_label: int = max(self.learning_map.keys())
        self.learning_map_np: np.ndarray = np.zeros((max_label + 1,), dtype=np.uint32)
        for raw_label, mapped_label in self.learning_map.items():
            self.learning_map_np[raw_label] = mapped_label

        # Collect all frame paths for selected sequences
        self.frame_paths: list[tuple[str, str]] = self._collect_frame_paths()

        self.last_seq = None
        self.last_frame_id = None

    def _collect_frame_paths(self) -> "list[tuple[str, str]]":
        """Collect all (sequence, frame_id) pairs from the dataset split."""
        frame_list = []
        for seq in self.sequences:
            seq_str = f"{int(seq):02d}"
            seq_velo_path = self.velodynes_path / seq_str / "velodyne"
            velo_files = sorted(seq_velo_path.glob("*.bin"))
            for file in velo_files:
                frame_list.append((seq_str, file.stem))
        return frame_list

    def __len__(self) -> int:
        """Return number of samples in the dataset split."""
        return len(self.frame_paths)

    def __iter__(self):
        self._iter_idx = 0
        return self

    def __next__(self):
        if self._iter_idx < len(self):
            item = self[self._iter_idx]
            self._iter_idx += 1
            return item
        else:
            raise StopIteration

    def _compute_scanline_ids(
        self, point_cloud: np.ndarray, n_scans: int = 64
    ) -> np.ndarray:
        """
        Approximate scanline indices based on point order.

        Args:
            point_cloud (np.ndarray): Nx3 array of 3D points.
            n_scans (int): Number of LiDAR scanlines (e.g., 64 for HDL-64E).

        Returns:
            np.ndarray: Nx1 array with estimated scanline indices (0 to n_scans - 1).
        """
        total_points = point_cloud.shape[0]
        scanline_ids = np.floor(
            np.linspace(0, n_scans, total_points, endpoint=False)
        ).astype(int)
        return scanline_ids.reshape(-1, 1)

    def get_last_seq_frame(self) -> tuple[str, str] | None:
        """
        Get the last sequence and frame ID loaded.

        Returns:
            tuple: (sequence, frame_id)
        """

        if self.last_seq is None or self.last_frame_id is None:
            return None
        else:
            return self.last_seq, self.last_frame_id

    def __getitem__(self, idx: int) -> "dict[str, np.ndarray] | np.ndarray":
        """
        Load a sample from the dataset.

        Args:
            idx (int): Index of the frame to load.

        Returns:
            tuple:
                - point_cloud_with_label (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id].
                - item_dict (dict): Contains 'point_cloud', 'label', and 'mask'.
        """

        seq, frame_id = self.frame_paths[idx]
        self.last_seq = seq
        self.last_frame_id = frame_id

        # Load point cloud (Nx4), drop reflectance
        velodyne_file_path = self.velodynes_path / seq / "velodyne" / f"{frame_id}.bin"
        with open(velodyne_file_path, "rb") as file:
            point_cloud = np.fromfile(file, dtype=np.float32).reshape(-1, 4)[:, :3]

        # Load and map semantic labels
        if not self.is_test:
            label_file_path = self.labels_path / seq / "labels" / f"{frame_id}.label"
            if label_file_path.exists():
                with open(label_file_path, "rb") as file:
                    raw_labels = np.fromfile(file, dtype=np.uint32) & 0xFFFF
                labels = self.learning_map_np[raw_labels]
                mask = labels != 0
            else:
                labels = np.zeros(point_cloud.shape[0], dtype=np.uint32)
                mask = np.ones(point_cloud.shape[0], dtype=bool)
        else:
            labels = np.zeros(point_cloud.shape[0], dtype=np.uint32)
            mask = np.ones(point_cloud.shape[0], dtype=bool)

        # Estimate scanline indices
        scanline_ids = self._compute_scanline_ids(point_cloud)

        if self.load_cluster:
            cluster_file_path = (
                self.clusters_path / seq / "clusters" / f"{frame_id}_cluster.label"
            )
            if cluster_file_path.exists():
                with open(cluster_file_path, "rb") as file:
                    clusters = np.fromfile(file, dtype=np.uint32)
                if clusters.shape[0] != point_cloud.shape[0]:
                    raise ValueError(
                        f"Predicted label count mismatch for frame {seq}/{frame_id}"
                    )
                clusters = clusters.reshape(-1, 1)
            else:
                clusters = np.zeros((point_cloud.shape[0], 1), dtype=np.uint32)
        else:
            clusters = np.zeros((point_cloud.shape[0], 1), dtype=np.uint32)

        # # Final format: [x, y, z, true_label, predicted_label, scanline_id]
        point_cloud_array = np.hstack(
            (point_cloud, labels.reshape(-1, 1), clusters, scanline_ids)
        )

        item_dict = {
            "point_cloud": point_cloud,
            "label": labels,
            "cluster": clusters,
            "scanline_id": scanline_ids,
            "mask": mask,
        }

        if self.return_np_array:
            return point_cloud_array
        else:
            return item_dict

# %% ../nbs/00_utils.ipynb 5
def pad_collate_fn(batch, max_clusters=32):
    """
    batch: lista de arrays (num_clusters_i, num_features)
    retorna:
        padded_batch: (batch_size, max_clusters, num_features)
        mask: (batch_size, max_clusters) -> 1 para vÃ¡lidos, 0 para padding
    """
    batch_size = len(batch)
    num_features = batch[0].shape[1]

    padded_batch = np.zeros((batch_size, max_clusters, num_features), dtype=np.float32)
    mask = np.zeros((batch_size, max_clusters), dtype=np.bool_)

    for i, item in enumerate(batch):
        num_clusters = min(item.shape[0], max_clusters)
        padded_batch[i, :num_clusters] = item[:num_clusters]
        mask[i, :num_clusters] = 1  # marca como "vÃ¡lido"

    return torch.tensor(padded_batch), torch.tensor(mask)

# %% ../nbs/00_utils.ipynb 7
class PointCloudVisualizer:
    def __init__(
        self,
        point_size: float = 1.0,
        grid_size=50,
        grid_spacing=1.0,
        grid_line_width=10,
        cluster_viz=False,
    ):
        """
        Visualizer class for rendering point clouds using Open3D with color-coded semantic labels.

        Args:
            point_size (float): Default size of points in the Open3D viewer.
        """
        self.point_size = point_size
        self.fixed_colors_rgb = self._get_fixed_colors_rgb()
        self.grid_size = grid_size
        self.grid_spacing = grid_spacing
        self.grid_line_width = grid_line_width
        self.cluster_viz = cluster_viz

    def set_point_sieze(self, point_size):
        self.point_size = point_size

    def set_grid_size(self, grid_size):
        self.grid_size = grid_size

    def set_grid_spacing(self, grid_spacing):
        self.grid_spacing = grid_spacing

    def set_grid_line_width(self, grid_line_width):
        self.grid_line_width = grid_line_width

    def _get_fixed_colors_rgb(self) -> "dict[int, list[float]]":
        fixed_colors = {
            -1: [255, 255, 255],  # plane
            0: [0, 0, 0],  # unlabeled
            1: [245, 150, 100],  # car
            2: [245, 230, 100],  # bicycle
            3: [150, 60, 30],  # motorcycle
            4: [180, 30, 80],  # truck
            5: [250, 80, 100],  # other-vehicle
            6: [30, 30, 255],  # person
            7: [200, 40, 255],  # bicyclist
            8: [90, 30, 150],  # motorcyclist
            9: [255, 0, 255],  # road
            10: [255, 150, 255],  # parking
            11: [75, 0, 75],  # sidewalk
            12: [75, 0, 175],  # other-ground
            13: [0, 200, 255],  # building
            14: [50, 120, 255],  # fence
            15: [0, 175, 0],  # vegetation
            16: [0, 60, 135],  # trunk
            17: [80, 240, 150],  # terrain
            18: [150, 240, 255],  # pole
            19: [0, 0, 255],  # traffic-sign
        }
        return {
            label: [c / 255.0 for c in reversed(rgb)]
            for label, rgb in fixed_colors.items()
        }

    def _get_color_map(self, labels: np.ndarray) -> np.ndarray:
        """
        Assigns RGB colors to labels.

        Args:
            labels (np.ndarray): Array of label ids.

        Returns:
            np.ndarray: Nx3 array of RGB colors.
        """
        color_map = np.zeros((labels.shape[0], 3))
        for i, label in enumerate(labels):
            if label in self.fixed_colors_rgb:
                color_map[i] = self.fixed_colors_rgb[label]
            else:
                rng = np.random.default_rng(label)
                color_map[i] = rng.random(3)
        return color_map

    def _get_color_map_clusters(
        self, n: int, color_type: str = "centroid"
    ) -> np.ndarray:
        """
        Return an Nx3 array of RGB values for cluster points.

        Args:
            n (int): Number of points.
            color_type (str): One of 'centroid', 'min', or 'max'.

        Returns:
            np.ndarray: Nx3 array of RGB colors.
        """
        if color_type == "centroid":
            return np.tile([1.0, 1.0, 1.0], (n, 1))  # white
        elif color_type == "min":
            return np.tile([0.0, 0.0, 1.0], (n, 1))  # blue
        elif color_type == "max":
            return np.tile([1.0, 0.0, 0.0], (n, 1))  # red
        else:
            raise ValueError(
                "Invalid color_type. Choose from 'centroid', 'min', or 'max'."
            )

    def _create_grid(self, grid_size=50, grid_spacing=1.0):
        points = []
        lines = []
        colors = []

        for i in range(-grid_size, grid_size + 1):
            # Linhas paralelas ao eixo X
            points.append([i * grid_spacing, -grid_size * grid_spacing, 0])
            points.append([i * grid_spacing, grid_size * grid_spacing, 0])
            lines.append([len(points) - 2, len(points) - 1])

            # Linhas paralelas ao eixo Y
            points.append([-grid_size * grid_spacing, i * grid_spacing, 0])
            points.append([grid_size * grid_spacing, i * grid_spacing, 0])
            lines.append([len(points) - 2, len(points) - 1])

            # Cores: mais clara a cada 10 unidades
            color = [0.3, 0.3, 0.3] if i % 10 else [0.6, 0.6, 0.6]
            colors.extend([color, color])

        grid = o3d.geometry.LineSet()
        grid.points = o3d.utility.Vector3dVector(points)
        grid.lines = o3d.utility.Vector2iVector(lines)
        grid.colors = o3d.utility.Vector3dVector(colors)
        return grid

    def _create_plane(self, normal_d_tuple, size=100.0):
        """
        Cria um plano baseado no vetor normal e no valor d.

        Args:
            normal_d_tuple (tuple): Tuple contendo o vetor normal (x, y, z) e o valor d.
            size (float): Tamanho do plano a ser desenhado.

        Returns:
            o3d.geometry.TriangleMesh: Mesh do plano.
        """
        normal, d = normal_d_tuple

        # Normalizar o vetor normal
        normal = np.array(normal)
        normal = normal / np.linalg.norm(normal)

        # Calcular 4 pontos do plano
        p1 = np.array(
            [-size, -size, -(normal[0] * (-size) + normal[1] * (-size) + d) / normal[2]]
        )
        p2 = np.array(
            [size, -size, -(normal[0] * size + normal[1] * (-size) + d) / normal[2]]
        )
        p3 = np.array(
            [size, size, -(normal[0] * size + normal[1] * size + d) / normal[2]]
        )
        p4 = np.array(
            [-size, size, -(normal[0] * (-size) + normal[1] * size + d) / normal[2]]
        )

        # Criar os pontos para o mesh
        points = np.vstack((p1, p2, p3, p4))

        # Criar os triÃ¢ngulos que formam o plano
        triangles = [[0, 1, 2], [0, 2, 3]]  # TriÃ¢ngulo 1  # TriÃ¢ngulo 2

        # Criar a malha triangular
        plane_mesh = o3d.geometry.TriangleMesh()
        plane_mesh.vertices = o3d.utility.Vector3dVector(points)
        plane_mesh.triangles = o3d.utility.Vector3iVector(triangles)

        # Colorir a malha do plano de azul
        plane_mesh.paint_uniform_color([0.45, 0.45, 0.45])

        return plane_mesh

    def _create_axis_arrow(self, length=1.0, color=[1, 0, 0], rotation=None):
        arrow = o3d.geometry.TriangleMesh.create_arrow(
            cylinder_radius=0.01,
            cone_radius=0.03,
            cylinder_height=length * 0.8,
            cone_height=length * 0.2,
        )
        arrow.compute_vertex_normals()
        arrow.paint_uniform_color(color)
        if rotation is not None:
            arrow.rotate(rotation, center=(0, 0, 0))
        return arrow

    def show(self, *args, **kwargs):
        if self.cluster_viz:
            self._show_clusters(*args, **kwargs)
        else:
            self._show_point_cloud(*args, **kwargs)

    def _show_clusters(
        self,
        point_cloud: dict,
        show_true_label: bool = False,
        show_pred_label: bool = False,
        show_grid: bool = False,
        show_min_max: bool = False,
        **kwargs,
    ) -> None:

        centroids = point_cloud[:, :3]
        true_label = point_cloud[:, -2]
        pred_label = point_cloud[:, -1]
        min_points = point_cloud[:, 3:6]
        max_points = point_cloud[:, 6:9]

        if show_true_label:
            colors = self._get_color_map(true_label)
            name = "- true label"
        elif show_pred_label:
            colors = self._get_color_map(pred_label)
            name = "- prediction label"
        else:
            colors = self._get_color_map_clusters(len(centroids), "centroid")
            name = ""
        colors_min = self._get_color_map_clusters(len(min_points), "min")
        colors_max = self._get_color_map_clusters(len(max_points), "max")

        vis = o3d.visualization.Visualizer()  # type: ignore
        vis.create_window(window_name=f"Cluster Visualization {name}", width=800, height=600)

        opt = vis.get_render_option()
        opt.point_size = self.point_size
        opt.background_color = np.asarray([0.1, 0.1, 0.1])
        opt.show_coordinate_frame = True
        opt.mesh_show_back_face = True

        # Eixos
        arrow_x = self._create_axis_arrow(
            length=1.0,
            color=[1, 0, 0],
            rotation=o3d.geometry.get_rotation_matrix_from_xyz([0, np.pi / 2, 0]),
        )
        arrow_y = self._create_axis_arrow(
            length=1.0,
            color=[0, 1, 0],
            rotation=o3d.geometry.get_rotation_matrix_from_xyz([-np.pi / 2, 0, 0]),
        )
        arrow_z = self._create_axis_arrow(length=1.0, color=[0, 0, 1], rotation=None)
        vis.add_geometry(arrow_x)
        vis.add_geometry(arrow_y)
        vis.add_geometry(arrow_z)

        if show_min_max:
            if min_points.size > 0:
                pcd_min = o3d.geometry.PointCloud()
                pcd_min.points = o3d.utility.Vector3dVector(min_points)
                pcd_min.colors = o3d.utility.Vector3dVector(colors_min)  # <-- aqui
                vis.add_geometry(pcd_min)

            if max_points.size > 0:
                pcd_max = o3d.geometry.PointCloud()
                pcd_max.points = o3d.utility.Vector3dVector(max_points)
                pcd_max.colors = o3d.utility.Vector3dVector(colors_max)  # <-- aqui
                vis.add_geometry(pcd_max)

        if centroids.size > 0:
            pcd_centroids = o3d.geometry.PointCloud()
            pcd_centroids.points = o3d.utility.Vector3dVector(centroids)
            pcd_centroids.colors = o3d.utility.Vector3dVector(colors)  # <-- aqui
            vis.add_geometry(pcd_centroids)

        if show_grid:
            vis.add_geometry(self._create_grid(self.grid_size, self.grid_spacing))
            opt.line_width = self.grid_line_width

        vis.run()
        vis.destroy_window()

    def _show_point_cloud(
        self,
        point_cloud: np.ndarray,
        normal_d_tuple: tuple | None = None,
        show_true_label: bool = False,
        show_ground: bool = True,
        show_clusters: bool = True,
        show_unlabeled: bool = True,
        show_plane: bool = False,
        show_grid: bool = False,
        **kwargs,
    ) -> None:
        """
        Visualize the filtered point cloud using Open3D.

        Args:
            point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].
        """

        label_col = 3 if show_true_label else 4
        name = "- true label" if show_true_label else "- cluster label"
        labels = point_cloud[:, label_col]

        # Apply filter mask
        mask = (
            (show_plane & (labels == -1))
            | (show_unlabeled & (labels == 0))
            | (show_ground & (labels == 9))
            | (show_clusters & (labels >= 1) & (labels != 9))
        )

        xyz = point_cloud[mask, :3]
        visible_labels = labels[mask].astype(int)
        colors = self._get_color_map(visible_labels)

        # Visualize with point size
        vis = o3d.visualization.Visualizer()  # type: ignore
        vis.create_window(window_name=f"Point cloud Visualization {name}", width=800, height=600)

        opt = vis.get_render_option()
        opt.point_size = self.point_size
        opt.background_color = np.asarray([0.1, 0.1, 0.1])  # estilo AutoCAD / pptk
        opt.show_coordinate_frame = True
        opt.mesh_show_back_face = True  # Exibir o lado de trÃ¡s do plano

        # extrair para uma funcao
        # X (vermelho): rotaciona -90Â° ao redor Z
        arrow_x = self._create_axis_arrow(
            length=1.0,
            color=[1, 0, 0],
            rotation=o3d.geometry.get_rotation_matrix_from_xyz([0, np.pi / 2, 0]),
        )
        # Y (verde): rotaciona +90Â° ao redor X
        arrow_y = self._create_axis_arrow(
            length=1.0,
            color=[0, 1, 0],
            rotation=o3d.geometry.get_rotation_matrix_from_xyz([-np.pi / 2, 0, 0]),
        )
        # Z (azul): jÃ¡ estÃ¡ na direÃ§Ã£o Z por padrÃ£o
        arrow_z = self._create_axis_arrow(length=1.0, color=[0, 0, 1], rotation=None)
        vis.add_geometry(arrow_x)
        vis.add_geometry(arrow_y)
        vis.add_geometry(arrow_z)

        # Create Open3D point cloud
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(xyz)
        pcd.colors = o3d.utility.Vector3dVector(colors)
        vis.add_geometry(pcd)

        if show_grid:
            vis.add_geometry(
                self._create_grid(self.grid_size, self.grid_spacing)
            )  # grid grande com espaÃ§amento 1m
            opt.line_width = self.grid_line_width
        if show_plane and normal_d_tuple != None:
            vis.add_geometry(self._create_plane(normal_d_tuple))

        vis.run()
        vis.destroy_window()

# %% ../nbs/00_utils.ipynb 8
def run_viz(point_cloud, normal_d_tuple=None, **visualizer_params):
    random_number = random.randint(10_000_000, 99_999_999)
    temp_dir = f"temp_vis{random_number}"
    os.makedirs(temp_dir, exist_ok=True)

    # Salvar nuvem de pontos
    np.save(os.path.join(temp_dir, "point_cloud.npy"), point_cloud)

    # Salvar plano (normal + d)
    if normal_d_tuple:
        normal_d = np.array([*normal_d_tuple[0], normal_d_tuple[1]])
        np.save(os.path.join(temp_dir, "normal_d.npy"), normal_d)

    # Salvar parÃ¢metros
    with open(os.path.join(temp_dir, "visualizer_config.json"), "w") as f:
        json.dump(visualizer_params, f)

    # Rodar visualizador em subprocesso chamando o prÃ³prio utils.py
    comando = ["python3", "../pfc_packages/utils.py", temp_dir]
    thread = threading.Thread(target=lambda: subprocess.run(comando))
    thread.start()

# %% ../nbs/00_utils.ipynb 9
def main_viz(temp_dir):
    try:
        # Carrega dados salvos no diretÃ³rio temporÃ¡rio
        point_cloud = np.load(os.path.join(temp_dir, "point_cloud.npy"))

        normal_d_path = os.path.join(temp_dir, "normal_d.npy")
        if os.path.exists(normal_d_path):
            normal_d_arr = np.load(normal_d_path)
            normal_d_tuple = (normal_d_arr[:3], normal_d_arr[3])
        else:
            normal_d_tuple = None

        with open(os.path.join(temp_dir, "visualizer_config.json")) as f:
            config = json.load(f)

        visualizer = PointCloudVisualizer(
            point_size=config.get("point_size", 1.0),
            grid_size=config.get("grid_size", 50),
            grid_spacing=config.get("grid_spacing", 1.0),
            grid_line_width=config.get("grid_line_width", 10),
            cluster_viz=config.get("cluster_viz", False),
        )

        visualizer.show(
            point_cloud,
            normal_d_tuple=normal_d_tuple,
            show_plane=config.get("show_plane", False),
            show_grid=config.get("show_grid", False),
            show_ground=config.get("show_ground", True),
            show_clusters=config.get("show_clusters", True),
            show_unlabeled=config.get("show_unlabeled", True),
            show_true_label=config.get("show_true_label", False),
            show_min_max=config.get("show_min_max", False),
            show_pred_label=config.get("show_pred_label", False),
        )

    finally:
        # Limpar diretÃ³rio temporÃ¡rio
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        main_viz(sys.argv[1])
    else:
        print("No temporary directory provided. Exiting.")

# %% ../nbs/00_utils.ipynb 11
def save_clusters(
    point_cloud: np.ndarray, seq: str, frame_id: str, output_base_path: str
) -> None:
    """
    Salva os rÃ³tulos preditos em formato .label seguindo a estrutura do SemanticKITTI.

    Args:
        point_cloud (np.ndarray): Array Nx6 com colunas [x, y, z, true_label, predicted_label, scanline_index].
        seq (str): NÃºmero da sequÃªncia (ex: '00', '01', ...).
        frame_id (str): ID do frame (ex: '000123').
        output_base_path (str): Caminho base atÃ© `data_odometry_cluster_pred/dataset/sequences`.
    """
    # Extrair coluna de prediÃ§Ã£o (Ã­ndice 4)
    predicted_labels = point_cloud[:, 4].astype(np.uint32)

    # Construir caminho de saÃ­da
    output_dir = Path(output_base_path) / seq / "clusters"
    output_dir.mkdir(parents=True, exist_ok=True)

    # Nome do arquivo com sufixo _pred.label
    output_file = output_dir / f"{frame_id}_cluster.label"

    # Salvar como arquivo binÃ¡rio .label
    predicted_labels.tofile(output_file)

# %% ../nbs/00_utils.ipynb 13
def verificar_consistencia_labels(pontos):
    """
    Verifica inconsistÃªncias entre rÃ³tulos verdadeiros e clusters atribuÃ­dos.
    TambÃ©m calcula estatÃ­sticas (mÃ©dia e desvio padrÃ£o) dos clusters inconsistentes.

    Entrada:
        pontos (np.ndarray): Array N x 6 contendo [x, y, z, true_label, pred_label, scanline_id].

    Retorna:
        - inconsistentes_total (int): NÃºmero de clusters inconsistentes encontrados.
        - combinacoes_contadas (Counter): Contagem de combinaÃ§Ãµes de labels inconsistentes.
        - estatisticas_erro (dict): MÃ©dias e desvios padrÃ£o das mÃ©tricas de erro, mais erro do cluster 9.
    """

    # Extrai o vetor de labels preditos dos pontos (coluna 4)
    labels_pred = pontos[:, 4].astype(int)  # Exemplo: [1, 1, 2, 2, 3]

    # Extrai o vetor de labels verdadeiros dos pontos (coluna 3)
    labels_true = pontos[:, 3].astype(int)  # Exemplo: [2, 2, 6, 6, 9]

    # Combina prediÃ§Ã£o e verdade em pares (pred_label, true_label) para cada ponto
    pares = np.stack((labels_pred, labels_true), axis=1)
    # Exemplo de pares:
    # [[1, 2],
    #  [1, 2],
    #  [2, 6],
    #  [2, 6],
    #  [3, 9]]

    # Cria um dicionÃ¡rio para mapear cada cluster predito para a lista de true_labels associados
    # Exemplo apÃ³s o preenchimento:
    # cluster_to_true_labels = {
    #     1: [2, 2],
    #     2: [6, 6],
    #     3: [9]
    # }
    cluster_to_true_labels = defaultdict(list)
    for cluster_id, true_label in pares:
        cluster_to_true_labels[cluster_id].append(true_label)

    # Lista para armazenar combinaÃ§Ãµes de labels inconsistentes
    combinacoes = []

    # Contador de clusters inconsistentes
    inconsistentes_total = 0

    # Listas para armazenar mÃ©tricas de erro dos clusters inconsistentes
    lista_total_pontos = []  # Exemplo: [10, 15]
    lista_pontos_label_dominante = []  # Exemplo: [7, 12]
    lista_erro_percentual = []  # Exemplo: [30.0, 20.0]

    # VariÃ¡vel para armazenar o erro percentual do cluster 9 (solo)
    erro_percentual_cluster_9 = None

    # Itera sobre cada cluster_id e sua lista de true_labels
    for cluster_id, labels in cluster_to_true_labels.items():

        # Ignora o cluster 9, pois serÃ¡ tratado separadamente
        if cluster_id == 9:
            continue

        # Remove pontos sem rÃ³tulo (label 0)
        labels_sem_unlabeled = [l for l in labels if l != 0]
        # Exemplo: se labels = [0, 2, 2, 3], labels_sem_unlabeled = [2, 2, 3]

        # Verifica se existem mÃºltiplos labels (inconsistÃªncia no cluster)
        #
        # labels_sem_unlabeled Ã© uma lista que contÃ©m os rÃ³tulos verdadeiros (true_labels) dos pontos de um cluster, ignorando os rÃ³tulos "0" (nÃ£o rotulados).
        #
        # A funÃ§Ã£o set() em Python transforma a lista em um conjunto, ou seja, remove todos os elementos repetidos.
        # Por exemplo:
        #   labels_sem_unlabeled = [2, 2, 2] â set(labels_sem_unlabeled) = {2} (apenas um elemento)
        #   labels_sem_unlabeled = [2, 2, 3] â set(labels_sem_unlabeled) = {2, 3} (dois elementos diferentes)
        #
        # Depois, usamos len(set(...)) para contar quantos elementos Ãºnicos existem.
        #
        # Se existir mais de 1 label diferente (len > 1), isso significa que o cluster estÃ¡ inconsistente â porque ele deveria ter apenas pontos de um Ãºnico tipo (um Ãºnico rÃ³tulo verdadeiro).
        if len(set(labels_sem_unlabeled)) > 1:
            inconsistentes_total += 1  # Incrementa o contador

            # Cria uma combinaÃ§Ã£o ordenada dos labels para contagem
            combinacao = tuple(sorted(set(labels_sem_unlabeled)))
            # Exemplo: (2, 3)

            combinacoes.append(combinacao)

            # EstatÃ­sticas para o cluster inconsistente:

            # NÃºmero total de pontos no cluster
            total_pontos = len(labels)

            # Conta quantos pontos existem para cada true_label
            #
            # labels_sem_unlabeled Ã© a lista de rÃ³tulos verdadeiros dos pontos dentro de um cluster (ignorando os 0s).
            #
            # Usamos Counter(labels_sem_unlabeled) para contar quantas vezes cada label aparece.
            # Counter Ã© uma ferramenta do Python que cria automaticamente um "dicionÃ¡rio de contagem".
            #
            # Exemplo:
            # labels_sem_unlabeled = [2, 2, 3]
            # EntÃ£o, contador_labels = Counter({2: 2, 3: 1})
            # Significa que o label 2 aparece 2 vezes e o label 3 aparece 1 vez.
            contador_labels = Counter(labels_sem_unlabeled)

            # Identifica o label dominante (aquele com mais pontos)
            #
            # Depois de contar quantos pontos existem para cada label, queremos saber qual label aparece mais vezes â o dominante.
            #
            # Usamos contador_labels.most_common(1) para pegar o label mais frequente.
            # Isso retorna uma lista com o (label, quantidade), e o [0] serve para pegar o primeiro (e Ãºnico) elemento dessa lista.
            #
            # Exemplo:
            # contador_labels = Counter({2: 2, 3: 1})
            # contador_labels.most_common(1) = [(2, 2)]
            # Ou seja:
            #   label_dominante = 2  (o label que mais aparece)
            #   pontos_label_dominante = 2  (quantas vezes ele aparece)
            label_dominante, pontos_label_dominante = contador_labels.most_common(1)[0]

            # Calcula o erro percentual (percentual de pontos errados)
            erro_percentual = 100 * (1 - (pontos_label_dominante / total_pontos))
            # Exemplo: erro_percentual = 100 * (1 - 2/3) â 33.33%

            # Armazena para cÃ¡lculo posterior de mÃ©dia e desvio padrÃ£o
            lista_total_pontos.append(total_pontos)
            lista_pontos_label_dominante.append(pontos_label_dominante)
            lista_erro_percentual.append(erro_percentual)

    # Conta quantas vezes cada combinaÃ§Ã£o de labels inconsistentes apareceu
    combinacoes_contadas = Counter(combinacoes)
    # Exemplo: Counter({(2, 3): 1, (6, 7): 2})

    # ---- CÃ¡lculo especial para o cluster 9 (chÃ£o) ----

    # Pega todos os true_labels associados ao cluster 9
    labels = cluster_to_true_labels[9]
    # Exemplo: labels = [9, 9, 9, 5] (erro no Ãºltimo)

    # NÃºmero total de pontos no cluster 9
    total_pontos_cluster_9 = len(labels)

    # Conta quantos pontos tÃªm erro (nÃ£o sÃ£o 9)
    num_erros_cluster_9 = sum(1 for l in labels if l != 9)

    # Calcula o erro percentual do cluster 9
    if total_pontos_cluster_9 > 0:
        erro_percentual_cluster_9 = 100 * (num_erros_cluster_9 / total_pontos_cluster_9)
        # Exemplo: 1 erro em 4 pontos â 25%
    else:
        erro_percentual_cluster_9 = 0.0

    # ---- CÃ¡lculo das estatÃ­sticas gerais ----

    # Se existirem clusters inconsistentes
    if inconsistentes_total > 0:
        estatisticas_erro = {
            "media_total_pontos": np.mean(
                lista_total_pontos
            ),  # MÃ©dia do total de pontos nos clusters inconsistentes
            "std_total_pontos": np.std(
                lista_total_pontos
            ),  # Desvio padrÃ£o do total de pontos
            "media_pontos_label_dominante": np.mean(
                lista_pontos_label_dominante
            ),  # MÃ©dia de pontos no label dominante
            "std_pontos_label_dominante": np.std(
                lista_pontos_label_dominante
            ),  # Desvio padrÃ£o dos pontos dominantes
            "media_erro_percentual": np.mean(
                lista_erro_percentual
            ),  # MÃ©dia dos erros percentuais
            "std_erro_percentual": np.std(
                lista_erro_percentual
            ),  # Desvio padrÃ£o dos erros percentuais
            "erro_percentual_cluster_9": erro_percentual_cluster_9,  # Erro no cluster do chÃ£o
        }
    else:
        # Se nÃ£o houver inconsistÃªncias, inicializa as mÃ©tricas com zero
        estatisticas_erro = {
            "media_total_pontos": 0,
            "std_total_pontos": 0,
            "media_pontos_label_dominante": 0,
            "std_pontos_label_dominante": 0,
            "media_erro_percentual": 0,
            "std_erro_percentual": 0,
            "erro_percentual_cluster_9": erro_percentual_cluster_9,
        }

    # Retorna o nÃºmero de inconsistÃªncias, as combinaÃ§Ãµes contadas e as estatÃ­sticas calculadas
    return inconsistentes_total, combinacoes_contadas, estatisticas_erro

# %% ../nbs/00_utils.ipynb 14
def get_statistics(idx, point_cloud):
    clusters = point_cloud[:, 4]
    num_clusters = len(np.unique(clusters))
    num_pontos = point_cloud.shape[0]

    cluster_sizes = [np.sum(clusters == label) for label in np.unique(clusters)]
    cluster_sizes_sorted = sorted(cluster_sizes, reverse=True)

    ground_cluster_size = np.sum(clusters == 9)
    largest_non_ground_cluster = (
        cluster_sizes_sorted[1]
        if cluster_sizes_sorted[0] == ground_cluster_size
        and len(cluster_sizes_sorted) > 1
        else cluster_sizes_sorted[0]
    )

    num_clusters_inconsistentes, combinacoes, erros_clusters = (
        verificar_consistencia_labels(point_cloud)
    )

    estatisticas_do_frame = {
        "frame_id": idx,
        "num_clusters": num_clusters,
        "num_pontos": num_pontos,
        "pontos_por_cluster_medio": (
            num_pontos / num_clusters if num_clusters > 0 else 0
        ),
        "largest_non_ground_cluster": largest_non_ground_cluster,
        "gound_cluster_size": ground_cluster_size,
        "num_clusters_inconsistentes": num_clusters_inconsistentes,  # Novo campo
        "combinacoes": combinacoes,
        "erros_clusters": erros_clusters,
    }

    return estatisticas_do_frame

# %% ../nbs/00_utils.ipynb 15
def compute_cluster_features(cluster_points, true_label):
    # Calcular centroide (3 primeiras posiÃ§Ãµes)
    centroid = np.mean(cluster_points[:, :3], axis=0)  # Somente as colunas x, y, z

    # Calcular extremos (mÃ¡ximo e mÃ­nimo) - 6 posiÃ§Ãµes
    max_point = np.max(cluster_points[:, :3], axis=0)
    min_point = np.min(cluster_points[:, :3], axis=0)

    # # Calcular desvio padrÃ£o em relaÃ§Ã£o ao centroide (3 posiÃ§Ãµes)
    # deviation = np.std(cluster_points[:, :3] - centroid, axis=0)

    # # Calcular a densidade (1 posiÃ§Ã£o)
    # # Verificar se o volume Ã© zero para evitar a divisÃ£o por zero
    # volume = np.prod(max_point - min_point)  # Volume da caixa delimitadora
    # if volume == 0:
    #     density = 0  # Ajusta densidade para 0 se volume for zero
    # else:
    #     density = len(cluster_points) / volume

    # # PCA para anÃ¡lise de planicidade, esfericidade, e elongaÃ§Ã£o (3 posiÃ§Ãµes)
    # # Apenas aplica PCA se houver 3 ou mais pontos
    # if len(cluster_points) >= 3:
    #     pca = PCA(n_components=3)  # Usando 3 componentes (se possÃ­vel)
    #     pca.fit(
    #         cluster_points[:, :3] - centroid
    #     )  # Subtrair o centroide para centralizar os dados
    #     eigenvalues = pca.explained_variance_

    #     planarity = (
    #         eigenvalues[1] / eigenvalues[0]
    #         if len(eigenvalues) > 1 and eigenvalues[0] != 0
    #         else 0
    #     )
    #     sphericity = (
    #         eigenvalues[0] / np.sum(eigenvalues)
    #         if len(eigenvalues) > 0 and np.sum(eigenvalues) != 0
    #         else 0
    #     )
    #     elongation = (
    #         eigenvalues[0] / eigenvalues[2]
    #         if len(eigenvalues) > 2 and eigenvalues[2] != 0
    #         else 0
    #     )
    # else:
    #     # Para clusters com menos de 3 pontos, podemos definir planicidade, esfericidade e elongaÃ§Ã£o como 0
    #     planarity = sphericity = elongation = 0

    # # CompactaÃ§Ã£o (1 posiÃ§Ã£o)
    # compactness = np.mean(np.linalg.norm(cluster_points[:, :3] - centroid, axis=1))

    # Criando o vetor de caracterÃ­sticas (concatenando todas as features)
    feature_vector = np.concatenate(
        [
            centroid,  # 3 posiÃ§Ãµes: [centroid_x, centroid_y, centroid_z]
            max_point,  # 3 posiÃ§Ãµes: [max_x, max_y, max_z]
            min_point,  # 3 posiÃ§Ãµes: [min_x, min_y, min_z]
            # deviation,  # 3 posiÃ§Ãµes: [dev_x, dev_y, dev_z]
            # [density],  # 1 posiÃ§Ã£o: [density]
            # [planarity],  # 1 posiÃ§Ã£o: [planarity]
            # [sphericity],  # 1 posiÃ§Ã£o: [sphericity]
            # [elongation],  # 1 posiÃ§Ã£o: [elongation]
            # [compactness],  # 1 posiÃ§Ã£o: [compactness]
            np.full_like(centroid.shape, true_label),
            np.zeros_like(centroid.shape),
        ]
    )

    return feature_vector

# %% ../nbs/00_utils.ipynb 16
def extract_features_from_clusters(
    point_cloud, drop_incosistentes=True, min_cluster_size=1
):
    """
    Extrai features dos clusters do point_cloud.
    - Ignora clusters inconsistentes (exceto cluster 9, se drop_incosistentes=True)
    - Ignora clusters onde todos os true_labels sÃ£o 0 (exceto cluster 9)
    - SÃ³ calcula features para clusters com pelo menos min_cluster_size pontos

    Args:
        point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id]
        drop_incosistentes (bool): Ignora clusters inconsistentes (exceto cluster 9)
        min_cluster_size (int): Tamanho mÃ­nimo do cluster para calcular features

    Returns:
        np.ndarray: Array de features dos clusters
    """
    clusters = np.unique(point_cloud[:, 4])
    features = []

    # PrÃ©-cÃ¡lculo para acelerar filtragem por cluster
    pred_labels = point_cloud[:, 4]
    true_labels = point_cloud[:, 3]

    for cluster_label in clusters:
        # Ignora clusters inconsistentes (exceto cluster 9)
        if cluster_label != 9:
            mask = pred_labels == cluster_label
            cluster_points = point_cloud[mask]
            # SÃ³ calcula features para clusters com tamanho suficiente
            if cluster_points.shape[0] < min_cluster_size:
                continue

            cluster_true_labels = true_labels[mask]
            unique_true_labels = np.unique(cluster_true_labels)
            if drop_incosistentes and len(unique_true_labels) > 1:
                continue

            # Ignora clusters onde todos os true_labels sÃ£o 0 (exceto cluster 9)
            if unique_true_labels[0] == 0:
                continue

        # Para cluster 9, sempre define true_label como 9
        cluster_true_label = 9 if cluster_label == 9 else unique_true_labels[0]

        features.append(
            compute_cluster_features(cluster_points[:, :3], cluster_true_label)
        )

    return np.array(features)

# %% ../nbs/00_utils.ipynb 18
def _plot_with_stats(
    ax,
    x,
    y,
    color,
    label,
    title,
    ylabel,
    linestyle="o",
    mean_fmt="{:.1f}",
    std_fmt="{:.1f}",
):
    """FunÃ§Ã£o auxiliar para plotar mÃ©tricas com mÃ©dia e desvio padrÃ£o."""
    media = np.mean(y)
    desvio = np.std(y)
    ax.plot(x, y, marker=linestyle, label=label, color=color)
    ax.axhline(
        media, color=color, linestyle="--", label=f"MÃ©dia = {mean_fmt.format(media)}"
    )
    ax.fill_between(
        x,
        media - desvio,
        media + desvio,
        color=color,
        alpha=0.2,
        label=f"Desvio = {std_fmt.format(desvio)}",
    )
    ax.set_title(title)
    ax.set_xlabel("Frame")
    ax.set_ylabel(ylabel)
    ax.legend()
    ax.grid(axis="y", color="gray", linestyle="--", linewidth=0.7, alpha=0.4)

# %% ../nbs/00_utils.ipynb 19
def plot_1(resumo_por_frame, seq_value=None):
    """
    Plota em 4 subplots verticais:
    1. NÃºmero total de pontos
    2. NÃºmero total de clusters
    3. Pontos por cluster mÃ©dio
    4. Porcentagem de reduÃ§Ã£o da dimensionalidade
    """
    num_clusters = np.array([frame["num_clusters"] for frame in resumo_por_frame])
    num_pontos = np.array([frame["num_pontos"] for frame in resumo_por_frame])
    pontos_por_cluster_medio = np.array(
        [frame["pontos_por_cluster_medio"] for frame in resumo_por_frame]
    )
    frames = np.arange(len(resumo_por_frame))
    reducao_dimensionalidade = 1 - (num_clusters / num_pontos)

    fig, axs = plt.subplots(4, 1, figsize=(12, 18))
    fig.tight_layout(pad=5.0)

    _plot_with_stats(
        axs[0],
        frames,
        num_pontos,
        "green",
        "NÃºmero de Pontos",
        "NÃºmero de Pontos por Frame",
        "NÃºmero de Pontos",
    )
    _plot_with_stats(
        axs[1],
        frames,
        num_clusters,
        "blue",
        "NÃºmero de Clusters",
        "NÃºmero de Clusters por Frame",
        "NÃºmero de Clusters",
    )
    _plot_with_stats(
        axs[2],
        frames,
        pontos_por_cluster_medio,
        "orange",
        "Pontos por Cluster MÃ©dio",
        "MÃ©dia de Pontos por Cluster por Frame",
        "Pontos por Cluster",
    )
    _plot_with_stats(
        axs[3],
        frames,
        reducao_dimensionalidade * 100,
        "purple",
        "ReduÃ§Ã£o Dimensionalidade (%)",
        "Porcentagem de ReduÃ§Ã£o da Dimensionalidade por Frame",
        "ReduÃ§Ã£o (%)",
        mean_fmt="{:.2f}",
        std_fmt="{:.2f}",
    )

    plt.tight_layout()
    plt.show()

    if seq_value is not None:
        plot_dir = Path("plots") / str(seq_value)
        plot_dir.mkdir(parents=True, exist_ok=True)
        fig.savefig(plot_dir / "plot-1.png", dpi=300, bbox_inches="tight")

# %% ../nbs/00_utils.ipynb 20
def plot_2(resumo_por_frame, seq_value=None):
    ground_cluster_size = [f["gound_cluster_size"] for f in resumo_por_frame]
    largest_non_ground_cluster = [
        f["largest_non_ground_cluster"] for f in resumo_por_frame
    ]
    frames = np.arange(len(resumo_por_frame))

    fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    _plot_with_stats(
        axs[0],
        frames,
        ground_cluster_size,
        "green",
        "Ground cluster",
        "Tamanho do Ground Cluster",
        "Tamanho",
    )
    _plot_with_stats(
        axs[1],
        frames,
        largest_non_ground_cluster,
        "blue",
        "Maior non ground cluster",
        "Tamanho do Maior non Ground Cluster",
        "Tamanho",
    )

    plt.tight_layout()
    plt.show()
    if seq_value is not None:
        plot_dir = Path("plots") / str(seq_value)
        plot_dir.mkdir(parents=True, exist_ok=True)
        fig.savefig(plot_dir / "plot-2.png", dpi=300, bbox_inches="tight")

# %% ../nbs/00_utils.ipynb 21
def plot_3(resumo_por_frame, seq_value=None):
    inconsistentes_por_frame = np.array(
        [frame["num_clusters_inconsistentes"] for frame in resumo_por_frame]
    )
    num_clusters_por_frame = np.array(
        [frame["num_clusters"] for frame in resumo_por_frame]
    )
    frames = np.arange(len(resumo_por_frame))

    porcentagem_erro = 1 - (inconsistentes_por_frame / num_clusters_por_frame)
    porcentagem_erro = np.clip(porcentagem_erro, 0, 1)

    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)
    _plot_with_stats(
        axs[0],
        frames,
        inconsistentes_por_frame,
        "red",
        "InconsistÃªncias por frame",
        "Clusters Inconsistentes por Frame",
        "NÂº de Clusters Inconsistentes",
        mean_fmt="{:.2f}",
        std_fmt="{:.2f}",
    )
    _plot_with_stats(
        axs[1],
        frames,
        porcentagem_erro,
        "green",
        "Porcentagem de acerto",
        "Porcentagem de Erro por Frame",
        "Porcentagem de Erro",
        mean_fmt="{:.2f}",
        std_fmt="{:.2f}",
    )

    plt.tight_layout()
    plt.show()
    if seq_value is not None:
        plot_dir = Path("plots") / str(seq_value)
        plot_dir.mkdir(parents=True, exist_ok=True)
        fig.savefig(plot_dir / "plot-3.png", dpi=300, bbox_inches="tight")

# %% ../nbs/00_utils.ipynb 22
def plot_4(resumo_por_frame, seq_value=None):
    """
    Plota sete mÃ©tricas de erro em subplots separados no mesmo figure.
    """
    # Cada mÃ©trica: (chave_media, chave_std, tÃ­tulo_mÃ©dia, tÃ­tulo_std, ylabel)
    metricas = [
        (
            "media_total_pontos",
            "std_total_pontos",
            "MÃ©dia: Total de Pontos nos Clusters Inconsistentes por Frame",
            "Desvio: Total de Pontos nos Clusters Inconsistentes por Frame",
            "Total de Pontos",
        ),
        (
            "media_pontos_label_dominante",
            "std_pontos_label_dominante",
            "MÃ©dia: Pontos do Label Dominante por Cluster Inconsistente",
            "Desvio: Pontos do Label Dominante por Cluster Inconsistente",
            "Pontos do Label Dominante",
        ),
        (
            "media_erro_percentual",
            "std_erro_percentual",
            "MÃ©dia: Erro Percentual nos Clusters Inconsistentes por Frame",
            "Desvio: Erro Percentual nos Clusters Inconsistentes por Frame",
            "Erro Percentual (%)",
        ),
        (
            "erro_percentual_cluster_9",
            None,
            "Erro Percentual no Cluster de Solo (Ground) por Frame",
            None,
            "Erro Percentual no Solo (%)",
        ),
    ]
    frames = np.arange(len(resumo_por_frame))
    fig, axs = plt.subplots(7, 1, figsize=(14, 28))
    fig.tight_layout(pad=5.0)
    plot_idx = 0

    for chave_media, chave_std, titulo_media, titulo_std, ylabel in metricas:
        # Plot mÃ©dia
        valores_media = np.array(
            [frame["erros_clusters"][chave_media] for frame in resumo_por_frame]
        )
        _plot_with_stats(
            axs[plot_idx],
            frames,
            valores_media,
            "red",
            f"{ylabel} (mÃ©dia) por frame",
            titulo_media,
            ylabel,
            mean_fmt="{:.2f}",
            std_fmt="{:.2f}",
        )
        plot_idx += 1
        # Plot std se existir
        if chave_std:
            valores_std = np.array(
                [frame["erros_clusters"][chave_std] for frame in resumo_por_frame]
            )
            _plot_with_stats(
                axs[plot_idx],
                frames,
                valores_std,
                "purple",
                f"{ylabel} (desvio) por frame",
                titulo_std,
                f"Desvio de {ylabel}",
                mean_fmt="{:.2f}",
                std_fmt="{:.2f}",
            )
            plot_idx += 1

    plt.tight_layout()
    plt.show()
    if seq_value is not None:
        plot_dir = Path("plots") / str(seq_value)
        plot_dir.mkdir(parents=True, exist_ok=True)
        fig.savefig(plot_dir / "plot-4.png", dpi=300, bbox_inches="tight")

# %% ../nbs/00_utils.ipynb 23
def plot_5(combinacoes_geral, seq_value=None, top_n=15):
    label_id_to_name = {
        -1: "plane",
        0: "unlabeled",
        1: "car",
        2: "bicycle",
        3: "motorcycle",
        4: "truck",
        5: "other-vehicle",
        6: "person",
        7: "bicyclist",
        8: "motorcyclist",
        9: "road",
        10: "parking",
        11: "sidewalk",
        12: "other-ground",
        13: "building",
        14: "fence",
        15: "vegetation",
        16: "trunk",
        17: "terrain",
        18: "pole",
        19: "traffic-sign",
    }

    combinacoes_ordenadas = sorted(
        combinacoes_geral.items(), key=lambda x: x[1], reverse=True
    )
    top_combinacoes = combinacoes_ordenadas[:top_n]

    def formatar_combinacao(combinacao):
        nomes = [label_id_to_name.get(int(l), f"desconhecido({l})") for l in combinacao]
        return ", ".join(nomes)

    labels_pizza = [formatar_combinacao(comb) for comb, _ in top_combinacoes]
    valores_pizza = [contagem for _, contagem in top_combinacoes]

    total = sum(combinacoes_geral.values())
    total_top = sum(valores_pizza)
    outros = total - total_top

    if outros > 0:
        labels_pizza.append("Outros")
        valores_pizza.append(outros)

    colors = cm.get_cmap("tab20")(np.linspace(0, 1, len(labels_pizza)))

    fig, ax = plt.subplots(figsize=(12, 10))

    # Calcula as porcentagens de cada fatia
    valores_pizza = np.array(valores_pizza)
    porcentagens = 100 * valores_pizza / valores_pizza.sum()

    # FunÃ§Ã£o para mostrar porcentagem sÃ³ se o slice for maior que 3%
    def autopct_format(pct):
        return ("%1.1f%%" % pct) if pct > 3 else ""

    wedges, texts, autotexts = ax.pie(  # type: ignore
        valores_pizza,
        labels=None,  # NÃ£o mostra os labels diretamente
        autopct=autopct_format,
        startangle=140,
        colors=colors,  # type: ignore
        pctdistance=0.8,
        wedgeprops=dict(edgecolor="w", linewidth=1),
        textprops=dict(fontsize=14, color="black"),
    )

    # Monta a legenda em colunas, porcentagens pequenas vÃ£o ao lado do nome
    legend_labels = []
    for nome, pct in zip(labels_pizza, porcentagens):
        legend_labels.append(f"{nome} ({pct:.1f}%)")

    # Calcula nÃºmero de colunas para legenda (mÃ¡x 5 por coluna)
    n_labels = len(legend_labels)
    ncol = min(3, max(1, (n_labels + 4) // 5))  # atÃ© 5 linhas por coluna

    ax.set_title(
        f"Top {top_n} combinaÃ§Ãµes inconsistentes + Outros", fontsize=20, pad=20
    )
    ax.axis("equal")

    # Legenda embaixo, em colunas, centralizada
    fig.legend(
        wedges,
        legend_labels,
        title="CombinaÃ§Ãµes",
        loc="lower center",
        bbox_to_anchor=(0.5, -0.08),
        fontsize=14,
        title_fontsize=16,
        frameon=False,
        ncol=ncol,
        columnspacing=1.5,
        handlelength=1.5,
    )

    plt.tight_layout(rect=[0, 0.08, 1, 1])  # type: ignore

    plt.show()
    if seq_value is not None:
        Path("plots").mkdir(parents=True, exist_ok=True)
        Path(f"plots/{seq_value}").mkdir(parents=True, exist_ok=True)
        fig.savefig(f"plots/{seq_value}/plot-5.png", dpi=300, bbox_inches="tight")
