[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pfc-semantic-segmentation",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "pfc-semantic-segmentation"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "pfc-semantic-segmentation",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall pfc_semantic_segmentation in Development mode\n# make sure pfc_semantic_segmentation package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to pfc_semantic_segmentation\n$ nbdev_prepare",
    "crumbs": [
      "pfc-semantic-segmentation"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "pfc-semantic-segmentation",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/Matheus-lla/pfc-semantic-segmentation.git\nor from conda\n$ conda install -c Matheus-lla pfc_semantic_segmentation\nor from pypi\n$ pip install pfc_semantic_segmentation\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "pfc-semantic-segmentation"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "pfc-semantic-segmentation",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "pfc-semantic-segmentation"
    ]
  },
  {
    "objectID": "gpf_slr.html",
    "href": "gpf_slr.html",
    "title": "Ground Plane Fitting and Scan Line Run for 3D LiDAR",
    "section": "",
    "text": "Imports\n\n\nGround Plane Fitting (GPF)\n\nsource\n\nextract_initial_seed_indices\n\n extract_initial_seed_indices (point_cloud:numpy.ndarray,\n                               num_points:int=1000,\n                               height_threshold:float=0.4)\n\n*Extract initial seed points for ground plane estimation (GPF).\nArgs: point_cloud (np.ndarray): N x 3 array of points (x, y, z). num_points (int): number of lowest Z points to average as LPR. height_threshold (float): threshold to select seeds close to LPR height.\nReturns: seeds_ids (np.ndarray): indices of points selected as initial seeds.*\n\nsource\n\n\nestimate_ground_plane\n\n estimate_ground_plane (points:numpy.ndarray)\n\n*Estimate the ground plane parameters using Singular Value Decomposition (SVD).\nArgs: points (np.ndarray): N x 3 array (x, y, z) of seed points assumed to be on or near the ground.\nReturns: tuple: - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane. - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).*\n\nsource\n\n\nrefine_ground_plane\n\n refine_ground_plane (point_cloud:numpy.ndarray, num_points:int=1000,\n                      height_threshold:float=0.4,\n                      distance_threshold:float=0.2, num_iterations:int=5)\n\n*Iteratively refine the ground plane estimation using seed points and distance threshold.\nArgs: point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id]. num_points (int): Number of lowest Z points used to compute the initial ground seed height (LPR). height_threshold (float): Vertical distance threshold from the LPR used to select initial seed points. distance_threshold (float): Max allowed point-to-plane distance for a point to be considered ground. num_iterations (int): Number of iterations to refine the plane and ground classification.\nReturns: tuple: - point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id], input array with ground points labeled. - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane. - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).*\n\n\n\nScan Line Run (SLR)\n\nsource\n\ngroup_by_scanline\n\n group_by_scanline (point_cloud:numpy.ndarray)\n\n*Group points by their scanline index in a vectorized way.\nArgs: point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].\nReturns: list[np.ndarray]: List of arrays. Each array contains the points (N_i x 6) from one scanline, sorted by scanline_id.*\n\nsource\n\n\nfind_runs\n\n find_runs (scanline_points:numpy.ndarray, distance_threshold:float=0.5)\n\n*Identify runs within a single scanline based on distance between consecutive points.\nArgs: scanline_points (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id]. distance_threshold (float): Distance threshold to consider two points part of the same run.\nReturns: list[np.ndarray]: List of arrays where each array contains the points of a run.*\n\nsource\n\n\nupdate_labels\n\n update_labels (runs_current:list[numpy.ndarray],\n                runs_above:list[numpy.ndarray], label_equivalences:dict,\n                merge_threshold:float=1.0)\n\n*Update labels of current scanline runs based on proximity to runs from previous scanline using KDTree.\nArgs: runs_current (list[np.ndarray]): List of N x 6 arrays for current scanline runs. runs_above (list[np.ndarray]): List of N x 6 arrays for previous scanline runs. label_equivalences (dict): Dictionary of label equivalences. merge_threshold (float): Maximum distance to consider connection between runs.*\n\nsource\n\n\nextract_clusters\n\n extract_clusters (scanlines:list[numpy.ndarray], label_equivalences:dict)\n\n*Apply resolved labels to all points and return a unified point cloud.\nArgs: scanlines (list[np.ndarray]): List of N x 6 arrays for each scanline. label_equivalences (dict): Dictionary of final label equivalences.\nReturns: np.ndarray: N x 6 array with updated labels in column 4.*\n\nsource\n\n\nscan_line_run_clustering\n\n scan_line_run_clustering (point_cloud:numpy.ndarray,\n                           distance_threshold:float=0.5,\n                           merge_threshold:float=1.0)\n\n*Perform scan line run clustering on non-ground points (predicted_label == 0).\nThis function detects connected components (runs) within scanlines, propagates and merges labels across scanlines, and assigns final labels to each point.\nArgs: point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, predicted_label, scanline_index]. distance_threshold (float): Distance threshold to consider two points part of the same run. merge_threshold (float): Maximum distance to consider connection between runs.\nReturns: np.ndarray: Point cloud with updated predicted labels (column 4).*",
    "crumbs": [
      "Ground Plane Fitting and Scan Line Run for 3D LiDAR"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Imports",
    "section": "",
    "text": "Dataset\n\nsource\n\nSemanticKittiDataset\n\n SemanticKittiDataset (data_path:str, split:str='train',\n                       load_cluster:bool=False, return_np_array:bool=True,\n                       sequence_list:list[int]|None=None)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\nsource\n\n\npad_collate_fn\n\n pad_collate_fn (batch, max_clusters=32)\n\nbatch: lista de arrays (num_clusters_i, num_features) retorna: padded_batch: (batch_size, max_clusters, num_features) mask: (batch_size, max_clusters) -&gt; 1 para válidos, 0 para padding\n\n\n\nVisualizer for Point Clouds\n\nsource\n\nPointCloudVisualizer\n\n PointCloudVisualizer (point_size:float=1.0, grid_size=50,\n                       grid_spacing=1.0, grid_line_width=10,\n                       cluster_viz=False)\n\n*Visualizer class for rendering point clouds using Open3D with color-coded semantic labels.\nArgs: point_size (float): Default size of points in the Open3D viewer.*\n\nsource\n\n\nrun_viz\n\n run_viz (point_cloud, normal_d_tuple=None, **visualizer_params)\n\n\nsource\n\n\nmain_viz\n\n main_viz (temp_dir)\n\n\n\n\nSave\n\nsource\n\nsave_clusters\n\n save_clusters (point_cloud:numpy.ndarray, seq:str, frame_id:str,\n                output_base_path:str)\n\n*Salva os rótulos preditos em formato .label seguindo a estrutura do SemanticKITTI.\nArgs: point_cloud (np.ndarray): Array Nx6 com colunas [x, y, z, true_label, predicted_label, scanline_index]. seq (str): Número da sequência (ex: ‘00’, ‘01’, …). frame_id (str): ID do frame (ex: ‘000123’). output_base_path (str): Caminho base até data_odometry_cluster_pred/dataset/sequences.*\n\n\n\nCluster processing\n\nsource\n\nverificar_consistencia_labels\n\n verificar_consistencia_labels (pontos)\n\n*Verifica inconsistências entre rótulos verdadeiros e clusters atribuídos. Também calcula estatísticas (média e desvio padrão) dos clusters inconsistentes.\nEntrada: pontos (np.ndarray): Array N x 6 contendo [x, y, z, true_label, pred_label, scanline_id].\nRetorna: - inconsistentes_total (int): Número de clusters inconsistentes encontrados. - combinacoes_contadas (Counter): Contagem de combinações de labels inconsistentes. - estatisticas_erro (dict): Médias e desvios padrão das métricas de erro, mais erro do cluster 9.*\n\nsource\n\n\nget_statistics\n\n get_statistics (idx, point_cloud)\n\n\nsource\n\n\ncompute_cluster_features\n\n compute_cluster_features (cluster_points, true_label, num_bins=5)\n\n\nsource\n\n\nextract_features_from_clusters\n\n extract_features_from_clusters (point_cloud, min_cluster_size=3,\n                                 num_bins=5)\n\n*Extrai features dos clusters do point_cloud. - Ignora clusters inconsistentes (exceto cluster 9, se drop_incosistentes=True) - Ignora clusters onde todos os true_labels são 0 (exceto cluster 9) - Só calcula features para clusters com pelo menos min_cluster_size pontos\nArgs: point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id] drop_incosistentes (bool): Ignora clusters inconsistentes (exceto cluster 9) min_cluster_size (int): Tamanho mínimo do cluster para calcular features\nReturns: np.ndarray: Array de features dos clusters*\n\n\n\nPlots\n\nsource\n\nplot_1\n\n plot_1 (resumo_por_frame, seq_value=None, smoothing_window=51)\n\nPlota em 4 subplots verticais com dados suavizados: 1. Número total de pontos 2. Número total de clusters 3. Pontos por cluster médio 4. Porcentagem de redução da dimensionalidade\n\nsource\n\n\nplot_2\n\n plot_2 (resumo_por_frame, seq_value=None)\n\n\nsource\n\n\nplot_3\n\n plot_3 (resumo_por_frame, seq_value=None, smoothing_window=51)\n\n\nsource\n\n\nplot_4\n\n plot_4 (resumo_por_frame, seq_value=None)\n\nPlota sete métricas de erro em subplots separados no mesmo figure.\n\nsource\n\n\nplot_5\n\n plot_5 (combinacoes_geral, seq_value=None, top_n=10)",
    "crumbs": [
      "Imports"
    ]
  },
  {
    "objectID": "pointnet.html",
    "href": "pointnet.html",
    "title": "Imports",
    "section": "",
    "text": "copia\n\nsource\n\nTNet\n\n TNet (k=3)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nPointNetEncoder\n\n PointNetEncoder (global_feat=True, feature_transform=False, channel=9)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nPointNetSeg\n\n PointNetSeg (num_classes, input_channels=9, feature_transform=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nfeature_transform_regulaizer\n\n feature_transform_regulaizer (trans)",
    "crumbs": [
      "Imports"
    ]
  }
]